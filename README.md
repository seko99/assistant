# Innokentiy - Речевой ассистент

Речевой ассистент на основе технологий машинного обучения, который слушает ключевые слова, записывает речь пользователя и отвечает на неё с помощью интеграции с локальными языковыми моделями и синтеза речи.

## Особенности

- **Детекция ключевых слов**: Использует Vosk для распознавания wake words
- **Распознавание речи**: FasterWhisper для точного преобразования речи в текст
- **Интеграция с LLM**: Поддержка локальных языковых моделей через LMStudio API
- **Синтез речи**: Silero TTS для естественного озвучивания текста
- **Интеллектуальная детекция пауз**: Автоматическое определение окончания фразы
- **Поддержка нескольких ключевых слов**: Настраиваемый список слов для активации
- **Сохранение контекста диалога**: Ведение истории разговора с пользователем
- **Конфигурируемость**: Гибкая настройка через JSON конфигурацию

## Архитектура

### Основные компоненты

- **`main.py`** - Главный файл приложения с классом `SpeechAssistant`
- **`core/wake_word.py`** - Детектор ключевых слов на основе Vosk
- **`core/speech_recognition.py`** - Распознаватель речи с использованием FasterWhisper
- **`core/llm_engine.py`** - Движок для работы с локальными языковыми моделями
- **`core/text_to_speech.py`** - Синтезатор речи на основе Silero TTS
- **`core/pause_detection.py`** - Детектор пауз в речи
- **`utils/`** - Вспомогательные утилиты и конфигурация

### Состояния системы

Ассистент работает в следующих состояниях:

1. **LISTENING** - Ожидание ключевого слова
2. **RECORDING** - Запись речи пользователя
3. **TRANSCRIBING** - Распознавание записанной речи
4. **PROCESSING** - Обработка текста с помощью LLM
5. **SYNTHESIZING** - Синтез и воспроизведение ответа

## Установка

### Требования

- Python 3.8+
- CUDA-совместимая видеокарта (опционально, для ускорения)
- LMStudio или другой совместимый сервер локальных языковых моделей (опционально)

### Зависимости

```bash
pip install -r requirements.txt
```

### Модель Vosk

Скачайте русскую модель Vosk:

```bash
wget https://alphacephei.com/vosk/models/vosk-model-small-ru-0.22.zip
unzip vosk-model-small-ru-0.22.zip
```

## Конфигурация

Настройки находятся в файле `config.json`:

### Ключевые параметры

```json
{
  "wake_word": {
    "keywords": ["иннокентий", "вергилий"],
    "model_path": "vosk-model-small-ru-0.22",
    "sample_rate": 16000,
    "pre_trigger_duration": 3.0
  },
  "transcription": {
    "whisper_model": "medium",
    "language": "ru",
    "device": "auto"
  },
  "assistant": {
    "max_recording_duration": 20,
    "tts_speaker": "aidar",
    "tts_sample_rate": 48000,
    "use_accentizer": true
  },
  "llm": {
    "enabled": true,
    "base_url": "http://127.0.0.1:1234/v1",
    "model": "local-model",
    "temperature": 0.7
  }
}
```

### Настройка ключевых слов

В секции `wake_word.keywords` можно указать список слов для активации ассистента:

```json
"keywords": ["иннокентий", "вергилий", "ассистент"]
```

### Настройка языковой модели

Секция `llm` управляет интеграцией с локальной языковой моделью:

- `enabled` - включить/выключить использование LLM (при отключении ассистент просто повторяет распознанный текст)
- `base_url` - адрес LMStudio сервера
- `model` - название модели (обычно "local-model" для LMStudio)
- `temperature` - творческий параметр модели (0.1-1.0)

### Настройка синтеза речи

- `tts_speaker` - голос диктора (aidar, baya, kseniya, xenia, eugene, tatyana)
- `use_accentizer` - использование автоматической расстановки ударений

## Использование

### Запуск

```bash
python main.py
```

### Опции командной строки

```bash
python main.py --debug    # Включить отладочный режим
python main.py --help     # Показать справку
```

### Процесс работы

1. Запустите LMStudio и загрузите языковую модель (опционально)
2. Запустите программу
3. Произнесите одно из ключевых слов (например, "Иннокентий")
4. После звукового сигнала произнесите вашу фразу
5. Ассистент распознает речь, обработает её через LLM и озвучит ответ
6. Система автоматически вернется к ожиданию ключевого слова

## Технические детали

### Используемые технологии

- **[Vosk](https://alphacephei.com/vosk/)** - Распознавание ключевых слов в реальном времени
- **[FasterWhisper](https://github.com/guillaumekln/faster-whisper)** - Быстрое и точное распознавание речи
- **[LMStudio](https://lmstudio.ai/)** - Запуск локальных языковых моделей
- **[OpenAI API](https://platform.openai.com/docs/api-reference)** - Совместимый интерфейс для работы с LLM
- **[Silero TTS](https://github.com/snakers4/silero-models)** - Качественный синтез русской речи
- **[RUAccent](https://github.com/TatianaShavrina/RUAccent)** - Автоматическая расстановка ударений

### Особенности реализации

- **Pre-trigger буфер**: Сохраняет аудио до обнаружения ключевого слова для лучшего качества записи
- **Интеллектуальная обработка диалогов**: Поддержка контекста разговора через LLM
- **Fallback режим**: При недоступности LLM ассистент продолжает работать, повторяя распознанный текст
- **Адаптивная детекция пауз**: Интеллектуальное определение окончания речи
- **Асинхронная обработка**: Многопоточная архитектура для плавной работы
- **Автоопределение устройств**: Автоматический выбор CUDA/CPU для оптимальной производительности

### Детекция пауз

Система использует анализ энергии сигнала для определения пауз в речи:
- Минимальная длительность записи: 0.5 сек
- Порог паузы: 2.0 сек тишины
- Максимальная длительность записи: 20 сек

## Разработка

### Структура проекта

```
assistant/
├── main.py                 # Основное приложение
├── config.json            # Конфигурация
├── requirements.txt       # Зависимости
├── core/                  # Основные модули
│   ├── __init__.py        # Инициализация модуля
│   ├── wake_word.py       # Детекция ключевых слов
│   ├── speech_recognition.py  # Распознавание речи
│   ├── llm_engine.py      # Движок языковых моделей
│   ├── text_to_speech.py  # Синтез речи
│   └── pause_detection.py # Детекция пауз
├── utils/                 # Утилиты
│   ├── __init__.py        # Инициализация модуля
│   ├── config.py          # Загрузка конфигурации
│   ├── config_keys.py     # Константы ключей конфигурации
│   ├── enums.py          # Перечисления состояний
│   └── audio_utils.py    # Аудио утилиты
└── vosk-model-small-ru-0.22/  # Модель Vosk
```

### Добавление новых функций

1. **Новые ключевые слова**: Добавьте слова в `config.json` → `wake_word.keywords`
2. **Новые голоса**: Измените `config.json` → `assistant.tts_speaker`
3. **Настройка LLM**: Измените параметры в секции `llm` для подключения к другим моделям
4. **Настройка чувствительности**: Измените параметры в секции `voice_detection`

## Устранение неполадок

### Общие проблемы

1. **Модель Vosk не найдена**
   - Убедитесь, что модель скачана и путь в конфигурации корректный

2. **Не работает микрофон**
   - Проверьте права доступа к микрофону
   - Убедитесь, что микрофон не используется другими приложениями

3. **Медленная работа**
   - Установите CUDA для ускорения
   - Уменьшите размер модели Whisper в конфигурации

4. **Плохое качество распознавания**
   - Убедитесь в хорошем качестве микрофона
   - Уменьшите фоновый шум
   - Увеличьте размер модели Whisper

5. **LLM не отвечает или работает медленно**
   - Убедитесь, что LMStudio запущен и модель загружена
   - Проверьте адрес `base_url` в конфигурации
   - Попробуйте уменьшить параметр `temperature`

### Отладка

Запустите с флагом `--debug` для получения подробных логов:

```bash
python main.py --debug
```

## Лицензия

Этот проект использует открытые модели и библиотеки. Убедитесь в совместимости лицензий при коммерческом использовании.

## Авторы

Проект разработан с использованием современных технологий машинного обучения для создания интуитивного речевого интерфейса.